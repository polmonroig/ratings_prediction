{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratings Prediction Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cm_i6KLH5y99"
   },
   "outputs": [],
   "source": [
    "# load all necessary libraries \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import regex \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EVy1JTWT54-6",
    "outputId": "f5661331-141c-410e-f4cc-b1ff280d322b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The shirt was more of a smock. I expected a so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The shirt was more of a smock. I expected a so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The shirt was more of a smock. I expected a so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just received my order today. When I opened th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First of all, the button hole on this belt nev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  RATING\n",
       "0  The shirt was more of a smock. I expected a so...       1\n",
       "1  The shirt was more of a smock. I expected a so...       1\n",
       "2  The shirt was more of a smock. I expected a so...       1\n",
       "3  Just received my order today. When I opened th...       1\n",
       "4  First of all, the button hole on this belt nev...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('../data/ratings.csv')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "wGBvGDMD8e7T",
    "outputId": "a0485f14-604a-4b54-fbba-5ff1e901e08a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.414284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RATING\n",
       "count  10000.000000\n",
       "mean       3.000000\n",
       "std        1.414284\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        3.000000\n",
       "75%        4.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEIdyJga7fO7"
   },
   "source": [
    "## Labels Analysis\n",
    "We can see that the labels from this dataset are discrete numbers from 1 to 5, thus this is a classification problem, more specifically, a sentiment analysis problem. From counting the labels we can see that the dataset is perfectly balanced that is great for training since the model won't be biased towards a specific class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pgB7LxRD6uUn",
    "outputId": "625fdf19-37c0-4166-883e-f5b9e8bc09ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2000, 2: 2000, 3: 2000, 5: 2000, 4: 2000})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(ratings['RATING']) \n",
    "ocurrences = collections.Counter(labels)\n",
    "ocurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEaK38rx7if-"
   },
   "source": [
    "## Variables analysis\n",
    "To analyse the words we need a tokenizer that can separate each sentence into a useful list of words. A tokenizer can be of many types, here we try 3 different ones. But in the future a better, more specialized tokenizer is needed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zOrdCIAo7k8U",
    "outputId": "7404a7c8-07f3-450a-95a6-2bef8329483e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bloody awful. I got to wash these twice on gentle cycle and already they're full of holes. So much for style and comfort...I got to wear these twice and they look like I was standing too close to a fire already. Swiss cheese pants!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first tokenizer is a simple python split, let's try it with an arbitrary sentence \n",
    "# \n",
    "# An obvious problem with this method is the inclusion of the '.' character in some words, \n",
    "# this can increase a lot the dictionary size of the dataset\n",
    "\n",
    "reviews = np.array(ratings['TEXT']) \n",
    "sample_review = reviews[42]\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "HVazlZwP-GJP",
    "outputId": "daddb981-bb3b-4498-b2e0-8cc5f2275dfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bloody', 'awful.', 'I', 'got', 'to', 'wash', 'these', 'twice',\n",
       "       'on', 'gentle', 'cycle', 'and', 'already', \"they're\", 'full', 'of',\n",
       "       'holes.', 'So', 'much', 'for', 'style', 'and', 'comfort...I',\n",
       "       'got', 'to', 'wear', 'these', 'twice', 'and', 'they', 'look',\n",
       "       'like', 'I', 'was', 'standing', 'too', 'close', 'to', 'a', 'fire',\n",
       "       'already.', 'Swiss', 'cheese', 'pants!'], dtype='<U11')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_review = reviews[42]\n",
    "sample_review = np.array(sample_review.split()) \n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "TDQ-YICnEyrg",
    "outputId": "95b9cc62-27ae-4528-b875-d9ce9cee8bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bloody', 'awful', 'I', 'got', 'to', 'wash', 'these', 'twice',\n",
       "       'on', 'gentle', 'cycle', 'and', 'already', \"they're\", 'full', 'of',\n",
       "       'holes', 'So', 'much', 'for', 'style', 'and', 'comfort', 'I',\n",
       "       'got', 'to', 'wear', 'these', 'twice', 'and', 'they', 'look',\n",
       "       'like', 'I', 'was', 'standing', 'too', 'close', 'to', 'a', 'fire',\n",
       "       'already', 'Swiss', 'cheese', 'pants', ''], dtype='<U8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try a more complex method , now the useess puntuation marks are gone\n",
    "# although we now have abbreviation issues such as wasn't \n",
    "sample_review = reviews[42]\n",
    "sample_review = np.array(regex.split(r'[-\\s.,;!?]+', reviews[42]))\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "F0oZWX39OxTT",
    "outputId": "2f9775f5-ba3a-4ecd-eeb7-fde72fc02c29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bloody', 'awful.', 'I', 'got', 'to', 'wash', 'these', 'twice',\n",
       "       'on', 'gentle', 'cycle', 'and', 'already', 'they', \"'re\", 'full',\n",
       "       'of', 'holes.', 'So', 'much', 'for', 'style', 'and', 'comfort',\n",
       "       '...', 'I', 'got', 'to', 'wear', 'these', 'twice', 'and', 'they',\n",
       "       'look', 'like', 'I', 'was', 'standing', 'too', 'close', 'to', 'a',\n",
       "       'fire', 'already.', 'Swiss', 'cheese', 'pants', '!'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An ever more complete tokenizer is the one included in the NLTK library, although this \n",
    "# creates more tokens because it keeps dots at the end it might be useful for the training \n",
    "sample_review = reviews[42]\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "sample_review = np.array(tokenizer.tokenize(sample_review))\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other text preprocessing methods such as case folding and removing stop words might help the model generalize and reduce the dimensions of the input, thus avoiding \"The curse of dimensionality\". The stop words are also downloaded from the NLTK library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9x9xIcVcQKbC",
    "outputId": "4386136e-b5f7-4117-8347-d072d6061324"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bloody', 'awful.', 'i', 'got', 'to', 'wash', 'these', 'twice',\n",
       "       'on', 'gentle', 'cycle', 'and', 'already', 'they', \"'re\", 'full',\n",
       "       'of', 'holes.', 'so', 'much', 'for', 'style', 'and', 'comfort',\n",
       "       '...', 'i', 'got', 'to', 'wear', 'these', 'twice', 'and', 'they',\n",
       "       'look', 'like', 'i', 'was', 'standing', 'too', 'close', 'to', 'a',\n",
       "       'fire', 'already.', 'swiss', 'cheese', 'pants', '!'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will apply case folding to reduce the vocabulary \n",
    "sample_review = np.array([token.lower() for token in sample_review])\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ApdFL2P4ReIC",
    "outputId": "9b6ba6a6-1e3a-4d41-d9cb-2c3d93dce4e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Bloody', 'awful.', 'I', 'got', 'wash', 'twice', 'gentle', 'cycle',\n",
       "       'already', \"'re\", 'full', 'holes.', 'So', 'much', 'style',\n",
       "       'comfort', '...', 'I', 'got', 'wear', 'twice', 'look', 'like', 'I',\n",
       "       'standing', 'close', 'fire', 'already.', 'Swiss', 'cheese',\n",
       "       'pants', '!'], dtype='<U8')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now for an extra preprocessign we want to remove stop words \n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "sample_review = np.array([token for token in sample_review if token not in stop_words]) \n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "P_zpJcI2-ca0",
    "outputId": "8d84eab7-8274-4864-f4bf-87f5249614b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encodings = encoder.fit_transform(sample_review.reshape(1, -1)) \n",
    "encodings.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUgnXi7ZA77p"
   },
   "source": [
    "**Before going any further we will perform a split train/test split to remove  my personal bias on the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5oC4Lt5_oli"
   },
   "outputs": [],
   "source": [
    "# I want to be sure that the datasets stay balanced to help the model into generalizing \n",
    "# now we will only work with the training set and we will never look the eval set \n",
    "# again \n",
    "X_train, X_eval, y_train, y_eval = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8Hao06UACaw4",
    "outputId": "a926fe5d-c708-4c02-b88e-51a947d3fa75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 26.283875\n",
      "std: 19.62603602321098\n",
      "min 1\n",
      "max 219\n"
     ]
    }
   ],
   "source": [
    "# now lets analyse the data further by calculating some global characteristics \n",
    "# of each sentence, such as the avg length. This is just an estimate becasuse the \n",
    "# sentence lenght dependes directly on the tokenizer we are using\n",
    "list_of_words = np.array(list(map(sentence_preprocessing, X_train)))\n",
    "reviews_lenghts = np.array([len(review) for review in list_of_words]) \n",
    "print('mean:', reviews_lenghts.mean())\n",
    "print('std:', reviews_lenghts.std())\n",
    "print('min',reviews_lenghts.min())\n",
    "print('max', reviews_lenghts.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVlcF70KLJdV"
   },
   "source": [
    "There are some sentences with a extremely low nubmer of words in it (i.e. 1 token). We should explore this extreme cases and evaluate if their are outliers that must be removed from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-4lhVuCrLImJ",
    "outputId": "cdc417ca-ce39-4875-ded8-1094acf8574f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I am 5'8\\\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print('Review:', X_train[reviews_lenghts.argmin()]) \n",
    "print('Label:', y_train[reviews_lenghts.argmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_yOBg-mI4Tc"
   },
   "outputs": [],
   "source": [
    "def sentence_preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentece, it returns a list \n",
    "    of tokens that have been preprocessed and filtered \n",
    "    using multiple strategies. \n",
    "    \"\"\"\n",
    "    # First we create the list of tokens \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "    # We then apply case folding \n",
    "    sentence = [token.lower() for token in sentence]\n",
    "    return [token for token in sentence if token not in stop_words] \n",
    "\n",
    "\n",
    "def create_bow(data):\n",
    "    \"\"\"\n",
    "    Given a list of sentences it creates and returns a \n",
    "    dataframe with prepared for training \n",
    "    \"\"\"\n",
    "\n",
    "    # Now lets create a bag of words using a simple split \n",
    "    bow = []\n",
    "    for sentence in data:\n",
    "        bow.append(Counter(sentence_preprocessing(sentence)))\n",
    "    \n",
    "    # Create a dataframe with all the records from the bag of words \n",
    "    df = pd.DataFrame.from_records(bow)\n",
    "    df = df.fillna(0).astype(int)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC9wV8NkWOBs"
   },
   "source": [
    "The Following dataframe represents the training dataset with the vocabulary size, the training set has a lenght of **8000** (80 percent of the original dataset). And a vocabulary of **7964** words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fAekw-8ZU_CB",
    "outputId": "ffa5b45a-af1f-4eda-cbd1-e009ba0f554a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8000, 7954)\n"
     ]
    }
   ],
   "source": [
    "df = create_bow(X_train)\n",
    "print('Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "AMTznTlDVLSo",
    "outputId": "48304d50-91cf-4e37-c727-fde4c7562c0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need</th>\n",
       "      <th>bra</th>\n",
       "      <th>minimizes</th>\n",
       "      <th>!</th>\n",
       "      <th>more.</th>\n",
       "      <th>although</th>\n",
       "      <th>back</th>\n",
       "      <th>cup</th>\n",
       "      <th>size</th>\n",
       "      <th>,</th>\n",
       "      <th>...</th>\n",
       "      <th>one\\nsmall</th>\n",
       "      <th>join</th>\n",
       "      <th>51</th>\n",
       "      <th>wrinkly.</th>\n",
       "      <th>lighting</th>\n",
       "      <th>i.</th>\n",
       "      <th>haute</th>\n",
       "      <th>couture</th>\n",
       "      <th>teens</th>\n",
       "      <th>woman.\\nretuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 7954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      need  bra  minimizes  !  more.  although  back  cup  size  ,  ...  \\\n",
       "0        2    2          1  1      1         1     1    1     1  2  ...   \n",
       "1        0    0          0  0      0         0     0    0     0  0  ...   \n",
       "2        0    2          0  2      0         0     0    1     1  1  ...   \n",
       "3        0    0          0  1      0         0     0    0     0  0  ...   \n",
       "4        0    0          0  0      0         0     0    0     0  0  ...   \n",
       "...    ...  ...        ... ..    ...       ...   ...  ...   ... ..  ...   \n",
       "7995     0    0          0  0      0         0     0    0     1  0  ...   \n",
       "7996     0    1          0  0      0         0     0    0     0  1  ...   \n",
       "7997     0    2          0  0      0         0     0    0     0  1  ...   \n",
       "7998     0    1          0  0      0         0     0    0     0  0  ...   \n",
       "7999     0    0          0  3      0         0     0    0     0  0  ...   \n",
       "\n",
       "      one\\nsmall  join  51  wrinkly.  lighting  i.  haute  couture  teens  \\\n",
       "0              0     0   0         0         0   0      0        0      0   \n",
       "1              0     0   0         0         0   0      0        0      0   \n",
       "2              0     0   0         0         0   0      0        0      0   \n",
       "3              0     0   0         0         0   0      0        0      0   \n",
       "4              0     0   0         0         0   0      0        0      0   \n",
       "...          ...   ...  ..       ...       ...  ..    ...      ...    ...   \n",
       "7995           0     0   0         0         0   0      0        0      0   \n",
       "7996           0     0   0         0         0   0      0        0      0   \n",
       "7997           0     0   0         0         0   0      0        0      0   \n",
       "7998           0     0   0         0         0   0      0        0      0   \n",
       "7999           0     0   0         0         0   0      0        0      0   \n",
       "\n",
       "      woman.\\nretuned  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "7995                0  \n",
       "7996                0  \n",
       "7997                0  \n",
       "7998                0  \n",
       "7999                0  \n",
       "\n",
       "[8000 rows x 7954 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to create an analysis on how the **TF** (term frequences) are in the dataset, for this we will use a `collections.Counter` to calculate the most used words in the entire corpus no just on a single review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G6BfUnavXL8P",
    "outputId": "9eedd95c-4a2e-4d06-f935-bc1aa44f46a4"
   },
   "outputs": [],
   "source": [
    "# Study term frequencies \n",
    "bow = []\n",
    "for sentence in X_train:\n",
    "    bow.append(Counter(sentence_preprocessing(sentence)))\n",
    "\n",
    "all_counters = collections.Counter()\n",
    "for counter in bow:\n",
    "    all_counters = all_counters + counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'need': 436,\n",
       "         'bra': 4103,\n",
       "         'minimizes': 19,\n",
       "         '!': 3977,\n",
       "         'more.': 53,\n",
       "         'although': 131,\n",
       "         'back': 1053,\n",
       "         'cup': 649,\n",
       "         'size': 2143,\n",
       "         ',': 11355,\n",
       "         'find': 461,\n",
       "         'straps': 923,\n",
       "         'could': 694,\n",
       "         'go': 363,\n",
       "         'little': 780,\n",
       "         'tighter': 53,\n",
       "         'even': 659,\n",
       "         'though': 310,\n",
       "         'tightest': 26,\n",
       "         \"'s\": 1454,\n",
       "         'tight': 388,\n",
       "         'would': 1745,\n",
       "         'prefer': 96,\n",
       "         'still': 518,\n",
       "         'works': 147,\n",
       "         '.': 5362,\n",
       "         'shoulder': 79,\n",
       "         'roll': 70,\n",
       "         'time': 659,\n",
       "         'making': 92,\n",
       "         'uncomfortable': 300,\n",
       "         'bali': 518,\n",
       "         '38dd.': 7,\n",
       "         'favorite': 144,\n",
       "         'ever': 203,\n",
       "         'ordered': 900,\n",
       "         '3': 289,\n",
       "         'site': 33,\n",
       "         'received': 197,\n",
       "         '(': 1069,\n",
       "         'marked': 30,\n",
       "         'item': 147,\n",
       "         '#': 59,\n",
       "         '3383': 4,\n",
       "         '38ddd': 15,\n",
       "         'previously': 27,\n",
       "         'purchased': 624,\n",
       "         'elsewhere': 13,\n",
       "         ')': 1065,\n",
       "         'least': 138,\n",
       "         '2': 469,\n",
       "         'sizes': 233,\n",
       "         'big': 364,\n",
       "         '+': 12,\n",
       "         '1': 122,\n",
       "         'band': 589,\n",
       "         '?': 286,\n",
       "         'disappointed': 348,\n",
       "         'frustrated.': 4,\n",
       "         'seconds/old': 4,\n",
       "         'stock': 15,\n",
       "         'changed': 59,\n",
       "         'sizing': 256,\n",
       "         'sad': 32,\n",
       "         'indication': 7,\n",
       "         'change': 71,\n",
       "         'site.': 8,\n",
       "         'waste': 89,\n",
       "         'money.': 26,\n",
       "         'returned': 370,\n",
       "         'recommend': 456,\n",
       "         'anyone': 92,\n",
       "         'wants': 31,\n",
       "         'save': 27,\n",
       "         'money': 172,\n",
       "         'buying': 183,\n",
       "         'new': 198,\n",
       "         'maternity': 133,\n",
       "         'pants.': 58,\n",
       "         'really': 821,\n",
       "         'helps': 53,\n",
       "         'make': 270,\n",
       "         'pants': 512,\n",
       "         'fit.': 186,\n",
       "         'like': 2025,\n",
       "         'use': 376,\n",
       "         'jeans': 187,\n",
       "         'dressy': 17,\n",
       "         'since': 340,\n",
       "         'buttons': 21,\n",
       "         'others': 109,\n",
       "         'hooks.': 28,\n",
       "         'love': 1143,\n",
       "         'great': 1139,\n",
       "         'saver': 25,\n",
       "         'bought': 1394,\n",
       "         'store': 225,\n",
       "         'them.': 265,\n",
       "         \"5'1\\\\\": 10,\n",
       "         'look': 494,\n",
       "         'feel': 467,\n",
       "         'sent': 73,\n",
       "         'sides': 153,\n",
       "         'poof': 4,\n",
       "         'thought': 279,\n",
       "         'try': 248,\n",
       "         'hoping': 127,\n",
       "         'no.': 5,\n",
       "         'designed': 39,\n",
       "         'criss-cross': 4,\n",
       "         'way': 462,\n",
       "         'good': 892,\n",
       "         'free': 244,\n",
       "         'sample': 42,\n",
       "         'exchange': 40,\n",
       "         'agreeing': 2,\n",
       "         'write': 20,\n",
       "         'review.': 16,\n",
       "         'softness': 10,\n",
       "         'material': 554,\n",
       "         'comfortable': 1271,\n",
       "         'fit': 1724,\n",
       "         'felt': 143,\n",
       "         'upon': 20,\n",
       "         'putting': 44,\n",
       "         'one': 1423,\n",
       "         'wearing': 640,\n",
       "         'seemed': 77,\n",
       "         'loosen': 18,\n",
       "         'bit.': 25,\n",
       "         'keep': 295,\n",
       "         'riding': 10,\n",
       "         'top': 464,\n",
       "         'knee': 23,\n",
       "         'pushing': 3,\n",
       "         'irritating.': 1,\n",
       "         'think': 358,\n",
       "         'looked': 186,\n",
       "         'told': 38,\n",
       "         'bunch': 61,\n",
       "         'different': 267,\n",
       "         'colors': 236,\n",
       "         'extremely': 139,\n",
       "         'satisfied': 62,\n",
       "         'far': 225,\n",
       "         'thinner': 60,\n",
       "         'expecting': 57,\n",
       "         'wear': 1514,\n",
       "         'crossed': 9,\n",
       "         'back.': 133,\n",
       "         'work': 450,\n",
       "         'long': 473,\n",
       "         'enough': 442,\n",
       "         'normal': 88,\n",
       "         'sticks': 34,\n",
       "         'sides.': 40,\n",
       "         'return': 362,\n",
       "         'plus': 131,\n",
       "         'pay': 35,\n",
       "         'shipping': 41,\n",
       "         'liked': 124,\n",
       "         'shirt.': 83,\n",
       "         'it\\\\': 14,\n",
       "         'pretty.': 14,\n",
       "         \"couldn\\\\'t\": 3,\n",
       "         'arms': 112,\n",
       "         'sleeves': 44,\n",
       "         'tight.': 82,\n",
       "         'get': 634,\n",
       "         'bigger': 117,\n",
       "         \"don\\\\'t\": 3,\n",
       "         'twig': 3,\n",
       "         \"''\": 35,\n",
       "         '}': 25,\n",
       "         '{': 22,\n",
       "         'r': 14,\n",
       "         ':': 283,\n",
       "         'id:47933895': 3,\n",
       "         'si:1': 10,\n",
       "         'v:3': 3,\n",
       "         't:3': 3,\n",
       "         'r:1': 3,\n",
       "         'h': 6,\n",
       "         'expected': 81,\n",
       "         'want': 203,\n",
       "         'comfort': 265,\n",
       "         'good.': 48,\n",
       "         'truly': 24,\n",
       "         'support': 838,\n",
       "         \"n't\": 2698,\n",
       "         'recommend.': 12,\n",
       "         'puckers': 7,\n",
       "         'hold': 227,\n",
       "         'well': 691,\n",
       "         'drift': 5,\n",
       "         \"'m\": 1163,\n",
       "         'dd': 156,\n",
       "         'heavy': 90,\n",
       "         '...': 670,\n",
       "         '..a': 5,\n",
       "         'large': 533,\n",
       "         '10': 86,\n",
       "         '12': 44,\n",
       "         'skirt': 60,\n",
       "         'fits': 596,\n",
       "         'medium.': 56,\n",
       "         'round': 21,\n",
       "         'hips/butt': 3,\n",
       "         \"'d\": 172,\n",
       "         'advise': 3,\n",
       "         'avoid': 33,\n",
       "         'bras': 967,\n",
       "         'run': 181,\n",
       "         'small.': 231,\n",
       "         '32c': 16,\n",
       "         'arrived': 86,\n",
       "         'small': 769,\n",
       "         'girls': 104,\n",
       "         'bra.': 479,\n",
       "         'ended': 112,\n",
       "         'giving': 49,\n",
       "         'niece.': 8,\n",
       "         'abut': 8,\n",
       "         'im': 37,\n",
       "         'worried': 40,\n",
       "         'stuck': 53,\n",
       "         'another': 280,\n",
       "         '34ddd': 6,\n",
       "         'always': 253,\n",
       "         'lookout': 6,\n",
       "         'supportive': 163,\n",
       "         'without': 310,\n",
       "         'underwires.': 6,\n",
       "         'fairly': 43,\n",
       "         'held': 38,\n",
       "         'well.': 166,\n",
       "         'generally': 22,\n",
       "         'stays': 61,\n",
       "         'poke': 39,\n",
       "         'may': 155,\n",
       "         'problem': 160,\n",
       "         'shorter': 72,\n",
       "         'women.': 23,\n",
       "         'disappointment': 46,\n",
       "         'cups': 510,\n",
       "         'means': 40,\n",
       "         'separating.': 6,\n",
       "         'larger': 300,\n",
       "         'ladies': 38,\n",
       "         'know': 167,\n",
       "         '-': 475,\n",
       "         'uniboob.': 14,\n",
       "         'drawing': 9,\n",
       "         'board': 6,\n",
       "         'perfect': 379,\n",
       "         '3-hook': 5,\n",
       "         'closure': 54,\n",
       "         'makes': 276,\n",
       "         'difference': 54,\n",
       "         'vs.': 9,\n",
       "         '2-hook': 5,\n",
       "         'closure.': 21,\n",
       "         'style': 470,\n",
       "         'day': 420,\n",
       "         'anxious': 9,\n",
       "         'take': 146,\n",
       "         'off.': 63,\n",
       "         'color': 406,\n",
       "         'selection': 13,\n",
       "         'awesome': 77,\n",
       "         'let': 74,\n",
       "         'fun': 32,\n",
       "         'playful': 5,\n",
       "         'choice': 52,\n",
       "         'two': 499,\n",
       "         'year': 119,\n",
       "         'old': 166,\n",
       "         'running': 42,\n",
       "         'around': 499,\n",
       "         'baby': 195,\n",
       "         'life': 90,\n",
       "         'saver.': 13,\n",
       "         'easy': 212,\n",
       "         'put': 294,\n",
       "         'able': 234,\n",
       "         'pump': 362,\n",
       "         'hassle': 21,\n",
       "         'holding': 65,\n",
       "         'suction': 23,\n",
       "         'breast': 151,\n",
       "         'bad': 115,\n",
       "         'reviews': 141,\n",
       "         'calling': 4,\n",
       "         'belly': 499,\n",
       "         'bandit': 209,\n",
       "         'poor': 74,\n",
       "         'compression.': 4,\n",
       "         'goes': 52,\n",
       "         'lotions': 9,\n",
       "         'getting': 167,\n",
       "         'dirty.': 4,\n",
       "         'shield': 28,\n",
       "         'slightly': 62,\n",
       "         'yellowed': 4,\n",
       "         'inside': 77,\n",
       "         'bandit.': 9,\n",
       "         'product': 496,\n",
       "         'help': 147,\n",
       "         'sale': 57,\n",
       "         'price': 279,\n",
       "         'unsure': 5,\n",
       "         'bronze': 7,\n",
       "         'neutral': 12,\n",
       "         'dark': 64,\n",
       "         'skin.': 28,\n",
       "         'tho': 7,\n",
       "         'slight': 32,\n",
       "         'shimmer': 3,\n",
       "         'see': 318,\n",
       "         'white': 193,\n",
       "         'tees': 22,\n",
       "         'hard': 154,\n",
       "         'person': 58,\n",
       "         'nude': 55,\n",
       "         'shade': 4,\n",
       "         'second': 126,\n",
       "         'true': 254,\n",
       "         'padding': 136,\n",
       "         'prevent': 34,\n",
       "         'showing': 30,\n",
       "         'certain': 36,\n",
       "         'areas.': 14,\n",
       "         'breasts': 237,\n",
       "         'pointed': 27,\n",
       "         'several': 214,\n",
       "         'layers': 38,\n",
       "         'appear': 16,\n",
       "         'cone': 32,\n",
       "         'like.': 19,\n",
       "         'everything': 138,\n",
       "         'place': 162,\n",
       "         'black': 284,\n",
       "         'stripes': 17,\n",
       "         'pleased.': 12,\n",
       "         'size.': 265,\n",
       "         'purchase': 216,\n",
       "         'say': 211,\n",
       "         '\\\\': 451,\n",
       "         'boyfriend': 9,\n",
       "         'v-necks': 8,\n",
       "         'target': 168,\n",
       "         'never': 187,\n",
       "         'disappoint': 4,\n",
       "         'me.': 293,\n",
       "         'colors.': 63,\n",
       "         'value': 38,\n",
       "         'esp.': 3,\n",
       "         \"'re\": 276,\n",
       "         'cut': 180,\n",
       "         'seems': 189,\n",
       "         'vary': 4,\n",
       "         'throughout': 34,\n",
       "         'year.': 8,\n",
       "         'shirts': 95,\n",
       "         'seem': 113,\n",
       "         'longer': 190,\n",
       "         'spring': 9,\n",
       "         'summer': 100,\n",
       "         'thicker': 32,\n",
       "         'fall.': 8,\n",
       "         'styles': 63,\n",
       "         'live': 33,\n",
       "         'texas.': 3,\n",
       "         'pretty': 394,\n",
       "         'mine': 56,\n",
       "         'pumping': 406,\n",
       "         'much': 824,\n",
       "         'easier': 75,\n",
       "         'using': 176,\n",
       "         'double': 91,\n",
       "         'must': 128,\n",
       "         'have.': 28,\n",
       "         'sock': 12,\n",
       "         'wish': 365,\n",
       "         '8': 89,\n",
       "         'stores': 62,\n",
       "         'line.': 21,\n",
       "         'also': 668,\n",
       "         'upsie': 40,\n",
       "         'best': 192,\n",
       "         'gifts': 9,\n",
       "         \"'ve\": 567,\n",
       "         'petite': 40,\n",
       "         '25': 9,\n",
       "         'weeks': 370,\n",
       "         'pains': 10,\n",
       "         'becoming': 16,\n",
       "         'difficult': 71,\n",
       "         'ignore.': 6,\n",
       "         'add': 55,\n",
       "         'sciatica': 6,\n",
       "         'soon': 38,\n",
       "         'walking': 55,\n",
       "         'hurt': 34,\n",
       "         'every': 235,\n",
       "         'step': 10,\n",
       "         'pain': 101,\n",
       "         'left': 56,\n",
       "         'hip': 29,\n",
       "         'knee.': 6,\n",
       "         'friend': 47,\n",
       "         'swear': 12,\n",
       "         'relief': 28,\n",
       "         'almost': 205,\n",
       "         'immediately.': 9,\n",
       "         'covered': 28,\n",
       "         'right': 370,\n",
       "         'amount': 55,\n",
       "         'lower': 57,\n",
       "         'lift': 71,\n",
       "         'took': 90,\n",
       "         'pressure.': 6,\n",
       "         'soft': 412,\n",
       "         'irritate': 16,\n",
       "         'skin': 126,\n",
       "         'heard': 13,\n",
       "         'women': 147,\n",
       "         'complain': 15,\n",
       "         'brands': 78,\n",
       "         'shift': 12,\n",
       "         'either.': 31,\n",
       "         'medical': 11,\n",
       "         'assistant': 6,\n",
       "         'constantly': 121,\n",
       "         'move': 76,\n",
       "         'sitting': 30,\n",
       "         'important': 12,\n",
       "         'recently': 94,\n",
       "         'glad': 96,\n",
       "         'did.': 29,\n",
       "         'side': 206,\n",
       "         'smaller': 264,\n",
       "         'buy': 438,\n",
       "         'bloody': 2,\n",
       "         'awful.': 27,\n",
       "         'got': 408,\n",
       "         'wash': 180,\n",
       "         'twice': 55,\n",
       "         'gentle': 20,\n",
       "         'cycle': 10,\n",
       "         'already': 96,\n",
       "         'full': 150,\n",
       "         'holes.': 5,\n",
       "         'standing': 17,\n",
       "         'close': 93,\n",
       "         'fire': 4,\n",
       "         'already.': 10,\n",
       "         'swiss': 3,\n",
       "         'cheese': 3,\n",
       "         'discontinued': 61,\n",
       "         'wire-free': 14,\n",
       "         '3389': 9,\n",
       "         'years': 315,\n",
       "         'ago.': 73,\n",
       "         'tried': 385,\n",
       "         'numerous': 32,\n",
       "         'trying': 145,\n",
       "         'replace': 63,\n",
       "         'ideal': 13,\n",
       "         'saw': 58,\n",
       "         'revolution': 15,\n",
       "         'wire': 126,\n",
       "         'thinking': 58,\n",
       "         'lacking': 12,\n",
       "         'description': 116,\n",
       "         'says': 62,\n",
       "         'molded.': 6,\n",
       "         'not.': 42,\n",
       "         'went': 141,\n",
       "         'hours': 65,\n",
       "         'metal': 23,\n",
       "         'strap': 130,\n",
       "         'rings': 15,\n",
       "         'starts': 22,\n",
       "         'digging': 20,\n",
       "         'sadly': 40,\n",
       "         'bright': 26,\n",
       "         'scuba': 4,\n",
       "         'fabric': 596,\n",
       "         'flattering.': 15,\n",
       "         'oh': 26,\n",
       "         'paper': 20,\n",
       "         'thin': 332,\n",
       "         'ran': 44,\n",
       "         'happy': 256,\n",
       "         'purchase.': 25,\n",
       "         'normally': 131,\n",
       "         '34': 61,\n",
       "         'f/g': 7,\n",
       "         'months': 208,\n",
       "         'pregnant': 204,\n",
       "         'milk': 70,\n",
       "         'comes': 126,\n",
       "         'wo': 131,\n",
       "         'extender': 45,\n",
       "         'came': 163,\n",
       "         'fitting': 83,\n",
       "         'cups.': 42,\n",
       "         'gives': 100,\n",
       "         'uni-boob': 7,\n",
       "         'point': 50,\n",
       "         'compared': 37,\n",
       "         'ones': 148,\n",
       "         'outweighs': 2,\n",
       "         'look.': 49,\n",
       "         'consider': 16,\n",
       "         \"'ll\": 192,\n",
       "         'chested': 21,\n",
       "         'woman': 122,\n",
       "         'happened': 52,\n",
       "         '3100.': 6,\n",
       "         'nicer': 22,\n",
       "         'one.': 131,\n",
       "         'chance': 37,\n",
       "         '3100': 6,\n",
       "         'produced': 6,\n",
       "         'something': 233,\n",
       "         'similar': 54,\n",
       "         'thank': 43,\n",
       "         'nice': 568,\n",
       "         'feel.': 9,\n",
       "         'isnt': 6,\n",
       "         'roomy': 22,\n",
       "         'playtex': 53,\n",
       "         'bras.': 120,\n",
       "         'ordering': 100,\n",
       "         'larger.': 15,\n",
       "         'made': 516,\n",
       "         'softer': 32,\n",
       "         'material.': 78,\n",
       "         'shoulders': 117,\n",
       "         'gift': 32,\n",
       "         'wife.': 4,\n",
       "         'loved': 172,\n",
       "         'it.': 557,\n",
       "         'said': 149,\n",
       "         'knowing': 17,\n",
       "         'quality': 383,\n",
       "         'product.': 23,\n",
       "         'laundered': 4,\n",
       "         'undergarments.': 2,\n",
       "         'fold': 30,\n",
       "         'away': 93,\n",
       "         'appeared': 5,\n",
       "         'shrunk.': 8,\n",
       "         'first': 531,\n",
       "         'uncomfortable.': 104,\n",
       "         'checked': 11,\n",
       "         'customer': 14,\n",
       "         'comfy': 225,\n",
       "         'forget': 14,\n",
       "         'fact': 88,\n",
       "         'lil': 7,\n",
       "         'bulgy': 3,\n",
       "         'cothing': 3,\n",
       "         '36': 54,\n",
       "         'tired': 24,\n",
       "         '38': 54,\n",
       "         'loose': 115,\n",
       "         'super': 330,\n",
       "         'cute.': 30,\n",
       "         \"5'2\": 19,\n",
       "         '120lbs': 9,\n",
       "         'quite': 171,\n",
       "         'pictured.': 13,\n",
       "         'jeans.': 38,\n",
       "         'come': 158,\n",
       "         '25+': 3,\n",
       "         'strings': 7,\n",
       "         'hanging': 28,\n",
       "         'package': 49,\n",
       "         'cheaply': 22,\n",
       "         'produced.': 2,\n",
       "         'job': 54,\n",
       "         'done': 60,\n",
       "         'for.': 48,\n",
       "         'cami': 24,\n",
       "         'comfortable.': 312,\n",
       "         'wil': 8,\n",
       "         'probably': 144,\n",
       "         'thanks': 31,\n",
       "         'going': 310,\n",
       "         'based': 73,\n",
       "         'reviews.': 35,\n",
       "         'hooks': 121,\n",
       "         'type': 111,\n",
       "         'anymore.': 27,\n",
       "         'lot.': 30,\n",
       "         'next': 148,\n",
       "         'line': 84,\n",
       "         'looks': 299,\n",
       "         'nipples': 69,\n",
       "         'show.': 11,\n",
       "         'flip': 14,\n",
       "         'flops': 7,\n",
       "         'comfortable-the': 2,\n",
       "         'sole': 8,\n",
       "         'cushioned': 13,\n",
       "         'arch': 5,\n",
       "         'sandal': 7,\n",
       "         'heel': 5,\n",
       "         'keeps': 59,\n",
       "         'foot': 23,\n",
       "         'eliminates': 5,\n",
       "         'toe': 6,\n",
       "         'fatigue': 2,\n",
       "         'occurs': 2,\n",
       "         'traditional': 8,\n",
       "         'flops.': 3,\n",
       "         'part': 115,\n",
       "         'reviewer': 33,\n",
       "         'mentioned-the': 2,\n",
       "         'uncomfortably': 6,\n",
       "         'mult-color': 2,\n",
       "         'design': 123,\n",
       "         'hope': 55,\n",
       "         'time.': 116,\n",
       "         'feet': 42,\n",
       "         'wet': 4,\n",
       "         'might': 120,\n",
       "         'embarrassing': 8,\n",
       "         'noise': 3,\n",
       "         'cute': 333,\n",
       "         'suit.': 6,\n",
       "         'daughter': 90,\n",
       "         'wears': 64,\n",
       "         '6.': 12,\n",
       "         'medium': 237,\n",
       "         'lot': 191,\n",
       "         'merona': 25,\n",
       "         'tops': 72,\n",
       "         'usually': 210,\n",
       "         'problems': 25,\n",
       "         'shirt': 299,\n",
       "         'right.': 38,\n",
       "         'short': 206,\n",
       "         'wide': 99,\n",
       "         'bottom.': 42,\n",
       "         'flattering': 98,\n",
       "         'flower': 52,\n",
       "         'underwire': 225,\n",
       "         '40': 37,\n",
       "         'years.': 100,\n",
       "         '--': 196,\n",
       "         '-no': 3,\n",
       "         'pointing': 10,\n",
       "         'breasts.': 38,\n",
       "         'bali.': 38,\n",
       "         'however': 449,\n",
       "         'definitely': 320,\n",
       "         'narrower': 10,\n",
       "         'prove': 5,\n",
       "         'less': 132,\n",
       "         'dress': 218,\n",
       "         'eve.': 2,\n",
       "         'thankfully': 7,\n",
       "         'month': 154,\n",
       "         'early': 10,\n",
       "         'send': 49,\n",
       "         'down.': 106,\n",
       "         'girl': 53,\n",
       "         ';': 220,\n",
       "         '18-20/2x': 1,\n",
       "         'tend': 50,\n",
       "         '20': 55,\n",
       "         'dresses': 33,\n",
       "         'wear.': 29,\n",
       "         'initially': 8,\n",
       "         'house': 72,\n",
       "         'ridiculously': 11,\n",
       "         'large.': 81,\n",
       "         'baggy': 45,\n",
       "         'boxy': 11,\n",
       "         'chest': 116,\n",
       "         'area': 136,\n",
       "         'thing': 174,\n",
       "         'experience': 31,\n",
       "         '44ds': 1,\n",
       "         'due': 93,\n",
       "         'kind': 96,\n",
       "         'there.': 15,\n",
       "         'shape': 189,\n",
       "         'all.\\\\ni': 1,\n",
       "         'sized': 113,\n",
       "         '18.': 1,\n",
       "         'baggy.': 8,\n",
       "         'improved': 2,\n",
       "         'large.\\\\ni': 1,\n",
       "         'ultimately': 4,\n",
       "         'nicely': 57,\n",
       "         '16': 13,\n",
       "         'absolutely': 83,\n",
       "         'red': 59,\n",
       "         'shoe': 22,\n",
       "         'lip.\\\\n\\\\nmoral': 1,\n",
       "         'story': 5,\n",
       "         'plan': 73,\n",
       "         'muscle': 4,\n",
       "         'beware': 15,\n",
       "         'dryer': 38,\n",
       "         'shrink.': 5,\n",
       "         'xl': 121,\n",
       "         'waist': 187,\n",
       "         \"5'4\\\\\": 27,\n",
       "         'open': 21,\n",
       "         'weave': 4,\n",
       "         'design.': 12,\n",
       "         'ordinarily': 7,\n",
       "         'online': 141,\n",
       "         'ca': 274,\n",
       "         'stores.': 24,\n",
       "         'ivory': 12,\n",
       "         'ago': 169,\n",
       "         'ok.': 27,\n",
       "         'free.': 32,\n",
       "         'hate': 71,\n",
       "         'highly': 131,\n",
       "         '1st': 33,\n",
       "         '2nd': 30,\n",
       "         'trimester.': 16,\n",
       "         '22': 23,\n",
       "         'pregnancy': 209,\n",
       "         'yet': 88,\n",
       "         'speak': 19,\n",
       "         '3rd': 28,\n",
       "         'trimester': 24,\n",
       "         'tummy': 63,\n",
       "         'band.': 28,\n",
       "         's/m': 22,\n",
       "         'pre-pregnancy': 105,\n",
       "         'typically': 32,\n",
       "         \"5'5\\\\\": 34,\n",
       "         'set': 74,\n",
       "         'men': 8,\n",
       "         'adorable': 39,\n",
       "         'christmas': 17,\n",
       "         'pajamas': 39,\n",
       "         '$': 103,\n",
       "         '29.99': 4,\n",
       "         'recently.': 3,\n",
       "         'child': 44,\n",
       "         'matching': 16,\n",
       "         'event': 3,\n",
       "         'coming': 43,\n",
       "         'up.': 185,\n",
       "         'washed': 217,\n",
       "         'ready': 28,\n",
       "         'dye': 19,\n",
       "         'adult': 15,\n",
       "         'pair': 243,\n",
       "         'bled': 14,\n",
       "         'parts': 22,\n",
       "         'santa': 3,\n",
       "         'etc': 29,\n",
       "         'pink': 58,\n",
       "         'followed': 18,\n",
       "         'washing': 102,\n",
       "         'instructions': 16,\n",
       "         'tag': 35,\n",
       "         'fine': 185,\n",
       "         'maybe': 142,\n",
       "         'polyester': 20,\n",
       "         'sure.': 3,\n",
       "         'goodness': 5,\n",
       "         'alone': 46,\n",
       "         'load': 8,\n",
       "         'clothes': 194,\n",
       "         'ruined.': 2,\n",
       "         'review': 105,\n",
       "         '17.99': 2,\n",
       "         'needless': 7,\n",
       "         'sure': 214,\n",
       "         'wonderful': 34,\n",
       "         'lasted.': 7,\n",
       "         'zipper': 295,\n",
       "         'broke': 84,\n",
       "         'worn': 288,\n",
       "         '18': 22,\n",
       "         'hour': 69,\n",
       "         'gel': 17,\n",
       "         'wirefree': 40,\n",
       "         'new.': 13,\n",
       "         'unfortunately': 165,\n",
       "         'all.': 129,\n",
       "         'edges': 32,\n",
       "         'stiff': 46,\n",
       "         'sharp': 21,\n",
       "         'slightest': 7,\n",
       "         'movement.\\\\nthankfully': 7,\n",
       "         'strange': 22,\n",
       "         'soft.': 42,\n",
       "         'see-through': 24,\n",
       "         'tank': 119,\n",
       "         'underneath.': 17,\n",
       "         'runs': 228,\n",
       "         \"5'3\\\\\": 23,\n",
       "         'texture': 10,\n",
       "         'apprehensive': 3,\n",
       "         '2s': 3,\n",
       "         'womens': 4,\n",
       "         'section': 41,\n",
       "         'actually': 199,\n",
       "         'functional': 25,\n",
       "         'zip': 66,\n",
       "         'want.': 11,\n",
       "         'narrow': 55,\n",
       "         'moves': 14,\n",
       "         'claimed.': 7,\n",
       "         'leisure': 23,\n",
       "         'such.': 7,\n",
       "         'order': 305,\n",
       "         'knew': 40,\n",
       "         'helpful': 30,\n",
       "         'imagine': 29,\n",
       "         'much.': 36,\n",
       "         'nicu': 8,\n",
       "         '4': 222,\n",
       "         'pumped': 23,\n",
       "         'useful': 13,\n",
       "         'home': 96,\n",
       "         'things': 135,\n",
       "         'mother': 29,\n",
       "         'multi-tasking.': 3,\n",
       "         'hands': 246,\n",
       "         'laundry': 10,\n",
       "         'eat': 16,\n",
       "         'dinner': 7,\n",
       "         'fully': 16,\n",
       "         'adjustable': 131,\n",
       "         'hands-free': 44,\n",
       "         'nursing/pumping': 3,\n",
       "         'moms': 28,\n",
       "         'matter': 50,\n",
       "         'bust': 103,\n",
       "         'looking': 298,\n",
       "         'tendency': 6,\n",
       "         'needs': 93,\n",
       "         'padding.': 29,\n",
       "         'bra.\\\\nvery': 9,\n",
       "         'discouraging': 10,\n",
       "         'front.': 34,\n",
       "         'stretchy.': 29,\n",
       "         'end': 124,\n",
       "         'supports': 26,\n",
       "         'in.': 50,\n",
       "         'excited': 69,\n",
       "         'sag': 7,\n",
       "         'slip': 73,\n",
       "         'house.': 20,\n",
       "         'better': 354,\n",
       "         'high': 158,\n",
       "         'waisted': 8,\n",
       "         'these.': 55,\n",
       "         'bands': 38,\n",
       "         'instead': 162,\n",
       "         'becuase': 8,\n",
       "         'bleed.': 2,\n",
       "         'minimizer': 141,\n",
       "         'boot': 4,\n",
       "         'extended': 8,\n",
       "         'fullest': 10,\n",
       "         'strangled.': 5,\n",
       "         'returning': 173,\n",
       "         'three': 116,\n",
       "         'best.': 13,\n",
       "         'decisions': 4,\n",
       "         'made.': 22,\n",
       "         'blessed': 4,\n",
       "         'workplace': 4,\n",
       "         'times': 192,\n",
       "         'frustrated': 9,\n",
       "         'shields.': 4,\n",
       "         'holds': 86,\n",
       "         'shields': 19,\n",
       "         'securely': 8,\n",
       "         'allows': 55,\n",
       "         'working': 81,\n",
       "         'pump.': 40,\n",
       "         'ask': 14,\n",
       "         'anything': 86,\n",
       "         'purchasing': 43,\n",
       "         'slacks/jeans': 2,\n",
       "         'nearly': 46,\n",
       "         'dreaded': 10,\n",
       "         'swimsuit': 21,\n",
       "         'hit': 11,\n",
       "         'ballpark': 2,\n",
       "         'seersucker': 2,\n",
       "         'ankle': 8,\n",
       "         'available': 74,\n",
       "         'face': 14,\n",
       "         'huge': 91,\n",
       "         'motivated': 1,\n",
       "         'determined': 9,\n",
       "         'work.': 44,\n",
       "         'level': 10,\n",
       "         'allowing': 2,\n",
       "         'relax': 4,\n",
       "         'bit': 357,\n",
       "         'computer': 22,\n",
       "         'read': 92,\n",
       "         'book': 14,\n",
       "         'snack': 5,\n",
       "         'whatever': 26,\n",
       "         'choose': 16,\n",
       "         'pumping.': 90,\n",
       "         'stretchy': 122,\n",
       "         'quality.': 66,\n",
       "         'yes': 32,\n",
       "         'believe': 66,\n",
       "         'sports': 67,\n",
       "         'worth': 158,\n",
       "         'opinion': 10,\n",
       "         'stars': 113,\n",
       "         'elastic': 193,\n",
       "         'stretched': 55,\n",
       "         'unbuttoned': 3,\n",
       "         'promised.': 3,\n",
       "         'used': 358,\n",
       "         'postpartum': 28,\n",
       "         'entire': 51,\n",
       "         'kit': 21,\n",
       "         'especially': 83,\n",
       "         'pooching': 5,\n",
       "         'support.': 171,\n",
       "         'ddd.': 16,\n",
       "         'boobs': 66,\n",
       "         'falling': 82,\n",
       "         'someone': 103,\n",
       "         'hurt.': 22,\n",
       "         'structure': 11,\n",
       "         'basic': 17,\n",
       "         'striped': 26,\n",
       "         'tee': 49,\n",
       "         'layering.': 10,\n",
       "         'attracted': 4,\n",
       "         'boat': 4,\n",
       "         'neck': 24,\n",
       "         'interesting': 4,\n",
       "         'tone': 13,\n",
       "         'grey': 54,\n",
       "         'stripe.': 4,\n",
       "         'spacing': 4,\n",
       "         'too.': 59,\n",
       "         'receive': 12,\n",
       "         'rating': 27,\n",
       "         'cling': 8,\n",
       "         'cotton': 179,\n",
       "         'fills': 4,\n",
       "         'hole': 50,\n",
       "         'wardrobe': 23,\n",
       "         'called': 24,\n",
       "         'couple': 197,\n",
       "         'finally': 60,\n",
       "         'denim': 24,\n",
       "         'wrinkle.': 6,\n",
       "         'reference': 24,\n",
       "         'm.': 2,\n",
       "         \"5'7\": 8,\n",
       "         '140': 9,\n",
       "         'lbs.': 12,\n",
       "         'tuck': 14,\n",
       "         'half': 80,\n",
       "         'seams': 55,\n",
       "         'apart.\\\\nthis': 9,\n",
       "         'within': 84,\n",
       "         'last': 224,\n",
       "         'barely': 82,\n",
       "         'lined': 44,\n",
       "         'b': 90,\n",
       "         'c': 78,\n",
       "         'ordered.': 16,\n",
       "         'around.': 22,\n",
       "         'perhaps': 40,\n",
       "         'pass': 14,\n",
       "         '38c.': 4,\n",
       "         'seam': 76,\n",
       "         'digs': 27,\n",
       "         'day.': 71,\n",
       "         'torso': 45,\n",
       "         'rolls': 94,\n",
       "         'lay': 53,\n",
       "         'flat': 53,\n",
       "         'market': 9,\n",
       "         'stretch': 212,\n",
       "         'pull': 61,\n",
       "         'shoulders.': 73,\n",
       "         'started': 124,\n",
       "         'unraveling.': 2,\n",
       "         'ears': 8,\n",
       "         ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining all counters of each review we can see the 10 most common tokens in the dataset. Unfortunately the most common one is the *','* symbol with a very big frequency of 11355, bigger than the number of instances in the entire dataset. That means that the comma does not provide any useful information at all, even if we suppose that is a *useful* token grammatically speaking, it is extremely repeated to the point that it has lost its purpose. And we have not taken into account ','s that are a postfix of a word. A lot of the following words are similar such as the dot. And as we will see later on the exclamation mark is biased. All this points to one thing, we must improve the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 11355),\n",
       " ('.', 5362),\n",
       " ('bra', 4103),\n",
       " ('!', 3977),\n",
       " (\"n't\", 2698),\n",
       " ('size', 2143),\n",
       " ('like', 2025),\n",
       " ('would', 1745),\n",
       " ('fit', 1724),\n",
       " ('wear', 1514)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counters.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-zpeY8SXMXg"
   },
   "source": [
    "## Model Training \n",
    "As for the training, the model has a little variance towards the training set as it is too espect. Also, after many test we can see that the accuracy of the training set is between 0.8 and 0.9 and the accuracy on the test set is between 0.7 and 0.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgZmnhhjXSVC"
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier = classifier.fit(df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9yBL0c7nXaMs",
    "outputId": "75190069-1183-4938-fc9b-d2a91e952286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86175"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(df)\n",
    "accuracy_score(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfA-X4dwaA-u"
   },
   "source": [
    "**Important, the new dataset has aproximately 300 new tokens that were not in the training set, that might afect the perfomance on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Ac9cUnlwXyet",
    "outputId": "40bce92f-0aa3-4ed7-eedf-240f1b6b2c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 5378)\n",
      "New bigger shape: (10000, 8295)\n",
      "New df with filtered cols shape: (2000, 7954)\n"
     ]
    }
   ],
   "source": [
    "# We need to make sure that columns from the training dataframe correspond the the exact same \n",
    "# cols in the test dataset \n",
    "df_test = create_bow(X_eval)\n",
    "print('Shape:', df_test.shape)\n",
    "all_bows = df.append(df_test)\n",
    "print('New bigger shape:', all_bows.shape)\n",
    "df_test = all_bows.iloc[len(df):][df.columns] # select only the test samples and the training cols \n",
    "print('New df with filtered cols shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l4InKT3iYrdg",
    "outputId": "3a430da4-b824-4b14-9a78-f6926b006c62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7645"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.fillna(0).astype(int)\n",
    "y_pred = classifier.predict(df_test)\n",
    "accuracy_score(y_eval,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics show that recall and precision have the same value as accuracy, if we had used lemmatization or stemming precision would probably be lower. A thing to take into account is that the mean absolute error is very small, this metric as well as some manual testing can tell us that even though the model does not calculate the exact rating always, it is generally off by 1 star (rating measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3fpoPcEkZ7dj",
    "outputId": "4388a74f-0845-4e70-a9c6-242643924c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 0.7645\n",
      "Precision score: 0.7645\n",
      "Mean absolute error: 0.3675\n"
     ]
    }
   ],
   "source": [
    "print('Recall score:', recall_score(y_eval, y_pred, average='micro')) \n",
    "print('Precision score:', precision_score(y_eval, y_pred, average='micro'))\n",
    "print('Mean absolute error:', mean_absolute_error(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the confusion matrix it is easy to rasure the previous statement, where labels are often off by one star. We can see that because most predictions are correct and the incorrect ones are adjecent to the predicted ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = confusion_matrix(y_eval, y_pred)\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(mat)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oGoy0Zv_cZ3g",
    "outputId": "94da0ca5-c1f6-4421-f54d-a6591d608b28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/classifier.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(classifier, '../data/classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RNZQcFC-lUXm"
   },
   "outputs": [],
   "source": [
    "classifier = load('../data/classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bNzz1nullaXF",
    "outputId": "c425dcb9-e401-470f-a37c-d6b8c6cc4a6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(df_test)\n",
    "accuracy_score(y_eval,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "For the classifier to be able to predict new unseed sentences we need to ensure that the preprocessed data has the same number of columnns in the same order than in the trainin set, to be able to do that later on (in api calls) we need to save a **sample dataframe** with all necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need</th>\n",
       "      <th>bra</th>\n",
       "      <th>minimizes</th>\n",
       "      <th>!</th>\n",
       "      <th>more.</th>\n",
       "      <th>although</th>\n",
       "      <th>back</th>\n",
       "      <th>cup</th>\n",
       "      <th>size</th>\n",
       "      <th>,</th>\n",
       "      <th>...</th>\n",
       "      <th>one\\nsmall</th>\n",
       "      <th>join</th>\n",
       "      <th>51</th>\n",
       "      <th>wrinkly.</th>\n",
       "      <th>lighting</th>\n",
       "      <th>i.</th>\n",
       "      <th>haute</th>\n",
       "      <th>couture</th>\n",
       "      <th>teens</th>\n",
       "      <th>woman.\\nretuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 7954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   need  bra  minimizes  !  more.  although  back  cup  size  ,  ...  \\\n",
       "0     0    0          0  1      0         0     0    0     0  2  ...   \n",
       "1     0    0          0  0      0         0     1    1     1  0  ...   \n",
       "\n",
       "   one\\nsmall  join  51  wrinkly.  lighting  i.  haute  couture  teens  \\\n",
       "0           0     0   0         0         0   0      0        0      0   \n",
       "1           0     0   0         0         0   0      0        0      0   \n",
       "\n",
       "   woman.\\nretuned  \n",
       "0                0  \n",
       "1                0  \n",
       "\n",
       "[2 rows x 7954 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ay_zK719leZ0"
   },
   "outputs": [],
   "source": [
    "df_test.iloc[:2].to_csv('../data/dataframe_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RatingsPrediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
